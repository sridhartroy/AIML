{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridhartroy/AIML/blob/main/Big_Data_A2_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.6 spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBIodAZSgBo0",
        "outputId": "d77558fe-680a-4baa-de40-1c7866283189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.6\n",
            "  Downloading pyspark-3.5.6.tar.gz (317.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/317.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.6) (0.10.9.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.6-py2.py3-none-any.whl size=317895798 sha256=bb00f8658bbb9e11f2f50761a116d7191111c442ecbe8dc6ae0ad8d66641f6fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/62/f3/ec15656ea4ada0523cae62a1827fe7beb55d3c8c87174aad4a\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "Successfully installed pyspark-3.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 3: Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NER-WordCount\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "0Zr3jFYzgMNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip -q smsspamcollection.zip\n",
        "\n",
        "# Read as RDD\n",
        "data_rdd = sc.textFile(\"SMSSpamCollection\")\n",
        "\n",
        "# Parse label and text\n",
        "def parse_line(line):\n",
        "    label, text = line.split(\"\\t\", 1)\n",
        "    return (label.strip(), text.strip())\n",
        "\n",
        "data_rdd = data_rdd.map(parse_line)\n",
        "print(\"Total records:\", data_rdd.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6PKGAy7emm_",
        "outputId": "be04caff-a1a1-4841-a760-1c75b74f9018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records: 5574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "token_pattern = re.compile(r\"[a-zA-Z']+\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    tokens = token_pattern.findall(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Transform dataset: (label, tokens)\n",
        "data_rdd = data_rdd.map(lambda x: (x[0], preprocess_text(x[1])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oza8ghnUeqpl",
        "outputId": "e59ed3f2-615c-49b6-d400-9dc847e3f039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_rdd, test_rdd = data_rdd.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Train size: {train_rdd.count()}, Test size: {test_rdd.count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUgrYgjtess1",
        "outputId": "60b34fc6-e3a1-4f87-a5e6-085841ca31f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 4458, Test size: 1116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Count docs per class\n",
        "class_doc_counts = train_rdd.map(lambda x: (x[0], 1)).reduceByKey(lambda a,b: a+b).collectAsMap()\n",
        "total_docs = sum(class_doc_counts.values())\n",
        "class_priors = {cls: math.log(count/total_docs) for cls, count in class_doc_counts.items()}\n",
        "\n",
        "# Word counts per class\n",
        "def word_count_mapper(record):\n",
        "    label, tokens = record\n",
        "    counts = Counter(tokens)\n",
        "    for word, c in counts.items():\n",
        "        yield ((label, word), c)\n",
        "\n",
        "word_counts = train_rdd.flatMap(word_count_mapper).reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "# Total words per class\n",
        "class_word_totals = word_counts.map(lambda x: (x[0][0], x[1])).reduceByKey(lambda a,b: a+b).collectAsMap()\n",
        "\n",
        "# Vocabulary size\n",
        "vocab = word_counts.map(lambda x: x[0][1]).distinct().collect()\n",
        "V = len(vocab)\n",
        "print(\"Vocab size:\", V)\n",
        "\n",
        "# Build log-prob table for each class\n",
        "alpha = 1.0\n",
        "class_word_probs = {}\n",
        "for (cls, word), count in word_counts.collect():\n",
        "    total = class_word_totals[cls]\n",
        "    denom = total + alpha * V\n",
        "    if cls not in class_word_probs:\n",
        "        class_word_probs[cls] = {}\n",
        "    class_word_probs[cls][word] = math.log((count + alpha) / denom)\n",
        "\n",
        "# Default prob for unseen words\n",
        "default_logprob = {cls: math.log(alpha / (class_word_totals[cls] + alpha * V)) for cls in class_word_totals}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Kd77p-eupG",
        "outputId": "c52836cd-6e37-43f2-a97f-a81693afc5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 5529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "def predict(tokens):\n",
        "    scores = {}\n",
        "    token_counts = Counter(tokens)\n",
        "    for cls in class_priors:\n",
        "        score = class_priors[cls]\n",
        "        for word, c in token_counts.items():\n",
        "            score += c * class_word_probs.get(cls, {}).get(word, default_logprob[cls])\n",
        "        scores[cls] = score\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for label, tokens in test_rdd.collect():\n",
        "    y_true.append(label)\n",
        "    y_pred.append(predict(tokens))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "print(\"Class priors (raw):\", {cls: math.exp(logp) for cls, logp in class_priors.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5tsLt-exET",
        "outputId": "42a6fdf3-7cb0-45f6-a935-fd693c611253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9740143369175627\n",
            "Confusion Matrix:\n",
            " [[965  24]\n",
            " [  5 122]]\n",
            "Class priors (raw): {'ham': 0.860924181247196, 'spam': 0.13907581875280395}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rz5qEt5SDwzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}