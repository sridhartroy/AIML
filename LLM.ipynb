{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPn1AapruwEgeqBsEQrY0u+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridhartroy/AIML/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdGSy1D7_h3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdf2543-6888-4758-d8d2-e95fdaf8e3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the file is :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# Read a publicly available text file from a URL.\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = (\"the-verdict.txt\")\n",
        "\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(\"Length of the file is : \", len(text))\n",
        "\n",
        "print(text[:99])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the text that was just read using reg expressions and print the length of the text before and after split\n",
        "\n",
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed), len(text))\n",
        "\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTSEtmq6SH0",
        "outputId": "82d8ae37-d912-43f8-c209-0698d7b5ce7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690 20479\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this step , we need to sort the tokenized text, remove dups, and assign an unique integer for each token.\n",
        "\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size, type(all_words))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek78eVxzCwQP",
        "outputId": "012be6cc-b3d6-4e7e-e0e3-b7921a03d10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130 <class 'list'>\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Class that takes in the vocab that we created. And also, we send a sample new text for tokenization and encoding to an unique integer id and then decode as well.\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "EKyP1B4oJJo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the above class by instantiating it with the vocabulary we created earlier from the verdict corpus. And then encode and decode\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "print(tokenizer.decode(ids))\n",
        "\n",
        "text1 = \"\"\"\"Mrs. said pride.\"\"\"\n",
        "ids1 = tokenizer.encode(text1)\n",
        "print(len(ids1), ids1)\n",
        "\n",
        "print(tokenizer.decode(ids1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZmZBq3nLw5j",
        "outputId": "8a094842-b3ce-486f-905e-785a644e9187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "6 [1, 67, 7, 851, 793, 7]\n",
            "\" Mrs. said pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what about words or tokens not in the corupus like below?\n",
        "\n",
        "text2 = \"\"\"\"Mr. Sridhar said pride.\"\"\"\n",
        "ids2 = tokenizer.encode(text2)\n",
        "print(len(ids2), ids2)\n",
        "\n",
        "print(tokenizer.decode(ids2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EgsnlXziO5iG",
        "outputId": "d5ce0dc0-46bb-40cf-db97-3357705f0aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Sridhar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5c4387239cc2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"\"Mr. Sridhar said pride.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mids2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5633fe271064>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5633fe271064>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Sridhar'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to add some additional tokens for a. unknown b. end of source text\n",
        "\n",
        "all_tokens = sorted(set(preprocessed))\n",
        "print(len(all_tokens))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "print(len(all_tokens))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab))\n",
        "\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVn4KnFopy_",
        "outputId": "83fd9f3c-a621-4a7c-9b91-018d1f52c293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "1132\n",
            "1132\n",
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now need to modify the tokenizer custom class to include above\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        print(\"Preprocessed before token check : \" , preprocessed)\n",
        "        # now check for each token in the preprocessed against the vocab.\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                             else \"<|unk|>\"\n",
        "                        for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "k5Q9kyRhXfWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the new tokenizer class\n",
        "\n",
        "# with existing valid text matching tokens in the vocab\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# 2 unrelated texts mixed with unknown tokens\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOq9HmF-ZhGr",
        "outputId": "a41613bc-45bb-4ecb-aabf-316f88b7769e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed before token check :  ['\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n",
            "Preprocessed before token check :  ['the', 'last', 'he', 'painted', ',', 'Sridhar', '<|endoftext|>', 'Hello', ',', 'do', 'you', 'like', 'tea', '?']\n",
            "14 [988, 602, 533, 746, 5, 1131, 1130, 1131, 5, 355, 1126, 628, 975, 10]\n",
            "the last he painted, <|unk|> <|endoftext|> <|unk|>, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using Byte Pair Encoding algorithm for Tokenization\n",
        "!pip install tiktoken\n",
        "\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yetXDTPedlWk",
        "outputId": "929b0dc5-9e21-4cd0-d44c-6dc854822a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEuDwprsIWTH",
        "outputId": "d8d67d20-94f1-4ddf-eed2-d1d9f8a4a6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1169, 938, 339, 13055, 11, 311, 6058, 9869, 220, 50256, 18435, 11, 466, 345, 588, 8887, 30]\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TextIO\n",
        "# Exercise 2.1 Byte pair encoding of unknown words\n",
        "\"\"\"\n",
        "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and print the individual token IDs. Then, call the decode function on each of the resulting integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input, “Akwirw ier.”\n",
        "\"\"\"\n",
        "tokenizerR50 = tiktoken.get_encoding(\"r50k_base\")\n",
        "tokenizerP50 = tiktoken.get_encoding(\"p50k_base\")\n",
        "tokenizerCl100k = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokenizero200k = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "text = \"Akwirw ier\"\n",
        "\n",
        "integers = tokenizerR50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"R50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerR50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerR50.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizerP50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"P50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerP50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerP50.decode(integers))\n",
        "\n",
        "integers = tokenizerCl100k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"cl100k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerCl100k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerCl100k.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizero200k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"o200k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizero200k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizero200k.decode(integers))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aL-zVEcsuOg",
        "outputId": "c02f8e15-fa56-4654-959e-83b8164a4ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "P50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "cl100k  [32, 29700, 404, 86, 602, 261] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 29700\n",
            "ir --> 404\n",
            "w --> 86\n",
            " i --> 602\n",
            "er --> 261\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "o200k  [32, 9500, 380, 86, 131455] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 9500\n",
            "ir --> 380\n",
            "w --> 86\n",
            " ier --> 131455\n",
            "---------------------------------\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# print(raw_text)\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "#print(len(enc_text), type(enc_text))\n",
        "\n",
        "# do a sampling for 50 tokens\n",
        "\n",
        "enc_sample = enc_text[50:]\n",
        "#print(enc_sample)\n",
        "#print(tokenizer.decode(enc_sample))\n",
        "\n",
        "context_size = 10\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, \"---->\", desired)\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo4YfNSsSinl",
        "outputId": "4e9c3e41-aab4-4a32-c899-48b9a99abee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n",
            "y:      [4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976]\n",
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n",
            "[290, 4920, 2241, 287, 257] ----> 4489\n",
            "[290, 4920, 2241, 287, 257, 4489] ----> 64\n",
            "[290, 4920, 2241, 287, 257, 4489, 64] ----> 319\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319] ----> 262\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262] ----> 34686\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686] ----> 41976\n",
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n",
            " and established himself in a ---->  vill\n",
            " and established himself in a vill ----> a\n",
            " and established himself in a villa ---->  on\n",
            " and established himself in a villa on ---->  the\n",
            " and established himself in a villa on the ---->  Riv\n",
            " and established himself in a villa on the Riv ----> iera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "btuivItP8qqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==2.4.0"
      ],
      "metadata": {
        "id": "6gf_tuVvg4KG",
        "outputId": "f67a2291-4ddd-4009-fbd7-814d42761e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "6f09785d9029435d96f1527b51e0cfdb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "w6jTzmraiGVS",
        "outputId": "3fb1cdfb-fa07-428e-c3c7-66f7af352fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor0d = torch.tensor(1)\n",
        "\n",
        "tensor1d = torch.tensor([1.1, 2, 3])\n",
        "\n",
        "tensor2d = torch.tensor([[1, 2,3],\n",
        "                         [3, 4,5]])\n",
        "\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]],\n",
        "                         [[5, 6], [7, 8]]])\n",
        "\n",
        "\n",
        "print(tensor0d)\n",
        "print(tensor1d)\n",
        "print(tensor2d)\n",
        "print(tensor3d)\n",
        "\n",
        "print(tensor1d.dtype)\n",
        "\n",
        "tensor0d = torch.tensor([1, 2, 3])\n",
        "print(tensor0d.dtype)\n",
        "\n",
        "tensor0df = tensor0d.to(torch.float32)\n",
        "print(tensor0df.dtype)\n",
        "print(tensor0d)\n",
        "print(tensor0df)"
      ],
      "metadata": {
        "id": "feB-C38YkqIM",
        "outputId": "4e22a89e-d1d6-4af5-eaca-2e35f449bb56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n",
            "tensor([1.1000, 2.0000, 3.0000])\n",
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.float32\n",
            "tensor([1, 2, 3])\n",
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor0d, tensor0d.shape)\n",
        "print(tensor1d, tensor1d.shape)\n",
        "print(tensor2d, tensor2d.shape)\n",
        "print(tensor3d, tensor3d.shape)\n",
        "\n",
        "print(tensor2d.reshape(3, 2))\n",
        "\n",
        "print(tensor2d.view(3, 2))\n",
        "\n",
        "\n",
        "print(tensor2d.T)"
      ],
      "metadata": {
        "id": "20Z21rH1mzzz",
        "outputId": "73e04407-f9e6-4cac-83b0-3a97a9e1e655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) torch.Size([3])\n",
            "tensor([1.1000, 2.0000, 3.0000]) torch.Size([3])\n",
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]]) torch.Size([2, 3])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]]) torch.Size([2, 2, 2])\n",
            "tensor([[1, 2],\n",
            "        [3, 3],\n",
            "        [4, 5]])\n",
            "tensor([[1, 2],\n",
            "        [3, 3],\n",
            "        [4, 5]])\n",
            "tensor([[1, 3],\n",
            "        [2, 4],\n",
            "        [3, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d)\n",
        "print(\"**\")\n",
        "print(tensor2d.T)\n",
        "print(\"MatMul\")\n",
        "print(tensor2d.matmul(tensor2d.T))\n",
        "print(tensor2d @ tensor2d.T)"
      ],
      "metadata": {
        "id": "cRCoMxfhoLvg",
        "outputId": "fd63ce92-fa29-4211-af52-f8a1947bc379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]])\n",
            "**\n",
            "tensor([[1, 3],\n",
            "        [2, 4],\n",
            "        [3, 5]])\n",
            "MatMul\n",
            "tensor([[14, 26],\n",
            "        [26, 50]])\n",
            "tensor([[14, 26],\n",
            "        [26, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing models as computational graphs.\n",
        "#  A logistic regression forward pass.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2])\n",
        "b = torch.tensor([0.0])\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "loss = F.binary_cross_entropy(a, y)"
      ],
      "metadata": {
        "id": "1FI38V8ipahX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the gradients via autograd function of torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)\n",
        "\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)\n",
        "\n",
        "print('******************************')\n",
        "\n",
        "loss.backward()\n",
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "iKZVPHmf2zHY",
        "outputId": "a0dea167-9227-4c2e-8763-fdc466fa3e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n",
            "******************************\n",
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a multi-layer perceptron with 2 hidden layers\n",
        "import torch.nn as M\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "01nQogMI_imX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the above neural network\n",
        "\n",
        "model = NeuralNetwork(50, 3)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# No. of trainable parameters of this model.\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable model parameters:\", num_params)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total number of model parameters:\", total_params)"
      ],
      "metadata": {
        "id": "kE1ZAEVQDTLO",
        "outputId": "201976ac-6cfd-498b-b296-7c313dd72de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "Total number of trainable model parameters: 2213\n",
            "Total number of model parameters: 2213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's print out the weight tensor\n",
        "\n",
        "print(\"Weight Matrix\")\n",
        "print(model.layers[0].weight, model.layers[0].weight.shape, type(model.layers), type(model.layers[0]), (model.layers[0].weight.dtype))\n",
        "\n",
        "print(\"Bias Vector\")\n",
        "print(model.layers[0].bias, model.layers[0].bias.shape, type(model.layers), type(model.layers[0]), (model.layers[0].bias.dtype))"
      ],
      "metadata": {
        "id": "Bf44s1RSIFDR",
        "outputId": "1d3eb57c-9881-4ada-8a5a-8472c533ad4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Matrix\n",
            "Parameter containing:\n",
            "tensor([[-0.1291,  0.0935,  0.0127,  ...,  0.0263, -0.1320, -0.0491],\n",
            "        [-0.1210,  0.1351,  0.1339,  ...,  0.0152, -0.0989,  0.1147],\n",
            "        [-0.0038,  0.1340,  0.0152,  ...,  0.1063,  0.0492,  0.0754],\n",
            "        ...,\n",
            "        [-0.1122,  0.0255,  0.0722,  ..., -0.0669, -0.0370, -0.0764],\n",
            "        [-0.0826, -0.1013, -0.0703,  ..., -0.1244,  0.0179,  0.0663],\n",
            "        [ 0.0561,  0.0303,  0.0830,  ...,  0.0114, -0.0295, -0.0990]],\n",
            "       requires_grad=True) torch.Size([30, 50]) <class 'torch.nn.modules.container.Sequential'> <class 'torch.nn.modules.linear.Linear'> torch.float32\n",
            "Bias Vector\n",
            "Parameter containing:\n",
            "tensor([ 0.0197,  0.0896, -0.0601,  0.0831,  0.1033, -0.0894,  0.0336,  0.0083,\n",
            "         0.0850,  0.0245,  0.1095, -0.0851,  0.0746, -0.0323,  0.0803,  0.1118,\n",
            "        -0.0947, -0.0363,  0.1379, -0.0381, -0.0640,  0.0015,  0.0638, -0.1206,\n",
            "        -0.0283,  0.1148,  0.1035,  0.1004, -0.1034,  0.0665],\n",
            "       requires_grad=True) torch.Size([30]) <class 'torch.nn.modules.container.Sequential'> <class 'torch.nn.modules.linear.Linear'> torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model)\n",
        "print(model.layers[0].weight.shape, model.layers[0].bias.shape)\n",
        "print(model.layers[2].weight.shape, model.layers[2].bias.shape)\n",
        "print(model.layers[4].weight.shape, model.layers[4].bias.shape)"
      ],
      "metadata": {
        "id": "UH397K9gMP-E",
        "outputId": "ad6a1b47-f8f4-4862-913a-d73d060f6df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([30, 50]) torch.Size([30])\n",
            "torch.Size([20, 30]) torch.Size([20])\n",
            "torch.Size([3, 20]) torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "X = torch.rand((1, 50))\n",
        "print(X)\n",
        "out = model(X)\n",
        "print(out)\n",
        "\n",
        "\n",
        "print(model.layers[0].weight, model.layers[0].bias)\n",
        "print(model.layers[2].weight, model.layers[2].bias)\n",
        "print(model.layers[4].weight, model.layers[4].bias)"
      ],
      "metadata": {
        "id": "k9a-6AU0Mljd",
        "outputId": "4683ebd4-1511-4005-89da-a43a042fb837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665, 0.1366, 0.1025, 0.1841,\n",
            "         0.7264, 0.3153, 0.6871, 0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274,\n",
            "         0.3821, 0.6605, 0.8536, 0.5932, 0.6367, 0.9826, 0.2745, 0.6584, 0.2775,\n",
            "         0.8573, 0.8993, 0.0390, 0.9268, 0.7388, 0.7179, 0.7058, 0.9156, 0.4340,\n",
            "         0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737, 0.4606,\n",
            "         0.5159, 0.4220, 0.5786, 0.9455, 0.8057]])\n",
            "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n",
            "Parameter containing:\n",
            "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
            "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
            "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
            "        ...,\n",
            "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
            "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
            "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
            "       requires_grad=True) Parameter containing:\n",
            "tensor([-0.1250,  0.0513,  0.0366,  0.0075,  0.0509,  0.0545, -0.0393,  0.0924,\n",
            "        -0.1412, -0.1232, -0.1063,  0.0081, -0.1249,  0.0101, -0.0019, -0.1298,\n",
            "         0.1388, -0.0330,  0.1017,  0.1247, -0.0554, -0.0417,  0.1388,  0.0159,\n",
            "         0.1215,  0.0385,  0.0769, -0.1224, -0.0279,  0.0991],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-1.0154e-02, -1.5861e-01,  1.9066e-02,  1.6987e-02, -1.7074e-02,\n",
            "          9.4865e-02,  7.1011e-02, -9.2097e-02, -3.2471e-02, -1.1231e-01,\n",
            "          8.6465e-02, -1.0711e-01, -9.6259e-02, -4.4155e-03,  1.2285e-01,\n",
            "          1.3708e-01, -1.5861e-01, -1.7688e-01,  1.3864e-01, -9.0875e-02,\n",
            "         -1.6941e-01, -1.3771e-02,  1.3315e-01, -2.3373e-02,  4.0696e-02,\n",
            "         -1.7768e-01,  1.5031e-01,  1.1774e-01,  1.2701e-01,  9.4861e-02],\n",
            "        [-9.3387e-02, -1.5303e-01,  6.9710e-04,  9.7921e-02, -7.9413e-02,\n",
            "          4.5254e-02, -1.4399e-02, -1.6972e-02,  4.7111e-02,  1.3326e-01,\n",
            "          3.8191e-02,  1.3138e-02,  2.9559e-02,  3.3591e-02, -5.9173e-02,\n",
            "         -2.0820e-02,  4.4497e-02, -1.4383e-01,  1.2064e-04, -1.6885e-01,\n",
            "         -1.6863e-01,  3.2975e-03,  2.5393e-02,  3.4459e-02, -6.7400e-02,\n",
            "         -2.0042e-02, -2.0234e-02, -6.0862e-03, -1.1799e-01,  3.9853e-02],\n",
            "        [ 5.6563e-02,  5.2087e-03, -1.4817e-02,  8.8404e-02,  1.2358e-01,\n",
            "         -2.2885e-03, -1.3856e-01, -1.2713e-01, -3.5647e-03, -1.6688e-01,\n",
            "         -1.3804e-01, -4.4271e-02, -7.3010e-02, -2.3431e-02,  4.4115e-02,\n",
            "          1.2497e-01,  1.8171e-01, -7.3016e-02, -1.7838e-01,  1.2122e-01,\n",
            "          1.5906e-01,  1.7420e-01,  4.1410e-02,  1.1633e-01,  4.3209e-04,\n",
            "         -1.2939e-01, -1.4691e-01, -1.0952e-01,  2.7672e-02, -4.6244e-02],\n",
            "        [ 4.6685e-02,  6.4175e-02, -1.1571e-01,  1.4561e-02,  1.7691e-01,\n",
            "          1.2027e-01, -2.0332e-02, -3.8157e-02, -7.1255e-02,  5.9831e-02,\n",
            "         -1.1019e-01,  1.8084e-01,  2.5788e-03, -1.1797e-01, -8.8593e-02,\n",
            "         -7.0314e-02, -1.4497e-01, -5.7067e-02, -1.7528e-01,  6.1810e-02,\n",
            "         -5.9462e-02,  4.4371e-02, -8.9423e-02, -8.2602e-02, -8.9245e-02,\n",
            "         -1.6231e-03, -1.3898e-01, -1.6236e-01,  3.7768e-02, -1.5703e-01],\n",
            "        [ 1.4778e-01,  1.3778e-01,  1.0478e-01,  1.2295e-01, -1.0692e-01,\n",
            "          2.7535e-02,  3.9353e-02,  1.5920e-01, -1.6470e-01, -5.2501e-02,\n",
            "          9.2477e-02,  9.6969e-02, -1.7690e-01,  1.6811e-01,  1.2099e-01,\n",
            "          1.5483e-01, -6.6973e-02, -9.2702e-02,  5.0032e-02,  1.3386e-01,\n",
            "          1.5885e-01, -1.1990e-01,  1.1666e-01, -2.9270e-02, -8.5924e-02,\n",
            "          1.0728e-01,  8.5550e-02,  4.0957e-02, -1.1469e-01,  1.7155e-01],\n",
            "        [-3.1611e-02,  5.5533e-03, -5.7722e-02, -1.5423e-01, -3.2761e-02,\n",
            "          3.6143e-02,  1.4452e-02, -4.2721e-02,  1.2813e-01, -2.6680e-02,\n",
            "         -1.3917e-01, -7.3217e-02, -4.3458e-02,  1.1988e-01,  1.4263e-01,\n",
            "          6.3297e-02,  1.7623e-01,  1.7438e-01,  4.1272e-02,  1.2515e-01,\n",
            "          7.8728e-02,  2.6415e-02,  1.6633e-01, -4.7062e-02, -1.4514e-01,\n",
            "         -1.6136e-01,  1.1267e-01, -1.2828e-01,  7.8414e-02, -1.0403e-02],\n",
            "        [-1.7202e-01, -5.8844e-02,  6.5513e-02,  5.7691e-04,  7.1033e-02,\n",
            "         -7.0540e-02, -8.3165e-02, -1.3339e-02, -1.6084e-01, -3.7179e-02,\n",
            "          1.0081e-02,  7.4172e-02, -5.6515e-02,  1.1535e-01, -4.6388e-03,\n",
            "         -4.7255e-02, -5.0935e-02,  1.2562e-01,  1.1003e-02, -1.0269e-01,\n",
            "         -1.0559e-01, -6.5035e-04,  4.2604e-02, -1.1221e-01, -1.6542e-01,\n",
            "          7.5141e-02,  5.5210e-02, -1.0214e-01, -9.9271e-02,  1.4185e-01],\n",
            "        [-1.1977e-01,  1.2830e-01, -1.1554e-01,  6.9452e-02, -9.3266e-03,\n",
            "          6.8769e-02,  8.1484e-02, -7.9705e-02, -1.6156e-01, -2.9207e-03,\n",
            "          3.1119e-02, -2.0498e-02,  6.6412e-02,  1.8065e-01, -7.4688e-02,\n",
            "          6.0297e-02,  5.7398e-02,  1.0436e-01, -7.7763e-02, -1.7681e-01,\n",
            "         -1.0666e-01,  1.2395e-02, -1.8109e-01,  1.5387e-01,  1.4700e-01,\n",
            "          4.2823e-02, -1.3201e-01, -1.7369e-01,  1.0825e-01, -1.2057e-01],\n",
            "        [ 6.2061e-02,  1.7306e-01, -1.0462e-01,  6.9222e-02, -9.5118e-03,\n",
            "          7.0322e-02,  8.6850e-02, -3.9505e-03,  3.2307e-02, -3.9305e-02,\n",
            "          4.9267e-02,  4.2660e-02,  1.1491e-01,  1.5994e-01,  4.9567e-02,\n",
            "          1.4213e-01, -1.7885e-01, -8.1386e-02, -1.6261e-01, -7.4337e-02,\n",
            "          1.4070e-01, -1.6133e-01,  1.0495e-01, -5.6505e-02,  8.8895e-03,\n",
            "         -1.1880e-01, -1.0216e-01,  1.6443e-02,  1.8018e-02, -1.5349e-01],\n",
            "        [-5.4879e-03, -6.9545e-02, -1.2516e-01, -1.5054e-01,  5.1067e-02,\n",
            "          2.3657e-02,  1.0642e-01,  1.4157e-01,  6.5826e-02, -6.0076e-02,\n",
            "          1.1329e-01,  5.6642e-02,  1.3649e-01,  9.6732e-02,  4.1889e-02,\n",
            "          9.0102e-02, -1.7953e-01,  9.4747e-02,  1.2196e-01, -2.4367e-02,\n",
            "         -1.5831e-01,  1.1654e-01,  9.3345e-03, -5.1091e-02, -2.8094e-02,\n",
            "          1.4220e-01, -6.0935e-02,  1.7947e-01,  1.3134e-01,  6.9098e-02],\n",
            "        [ 9.5976e-02, -1.2119e-01, -1.5018e-01,  1.3293e-01,  1.1696e-01,\n",
            "         -1.4044e-01, -1.1161e-01,  7.6049e-02,  5.9951e-02,  1.1501e-01,\n",
            "         -1.2624e-01,  9.0305e-02,  1.5189e-02, -1.3900e-01,  1.8119e-01,\n",
            "         -3.3564e-02, -7.9913e-02, -1.5007e-01, -6.2586e-02, -8.8812e-02,\n",
            "         -6.6300e-03, -1.3452e-01, -1.7013e-01, -1.7279e-01, -1.0307e-01,\n",
            "         -6.6781e-02,  2.3535e-02,  4.4560e-02,  1.0916e-01, -1.2035e-01],\n",
            "        [ 1.1683e-01,  1.1265e-01, -1.5032e-01, -1.2258e-01, -9.9277e-02,\n",
            "         -8.5493e-03, -1.7132e-01,  2.6558e-02,  7.4480e-02,  1.3092e-01,\n",
            "          1.1298e-01, -9.1413e-02, -1.2121e-01,  1.0977e-01, -1.5265e-02,\n",
            "         -1.0690e-01,  1.0925e-01,  9.0309e-02,  1.0382e-01, -8.3814e-02,\n",
            "         -2.9132e-02,  1.1034e-01, -1.8141e-01, -1.7327e-01, -1.2493e-01,\n",
            "          8.6094e-02, -1.7258e-01,  7.4506e-02, -1.1579e-01, -5.9486e-02],\n",
            "        [-6.1575e-02,  1.0901e-01,  1.5457e-01,  1.0825e-01, -1.2382e-01,\n",
            "         -2.0479e-02,  9.7674e-02,  1.0308e-01, -8.3447e-02,  8.9352e-02,\n",
            "         -6.6935e-03, -1.4226e-01,  8.3289e-02,  3.7189e-03,  8.2901e-02,\n",
            "          1.3366e-01,  1.0924e-01,  4.2660e-02, -1.2721e-02, -3.9557e-02,\n",
            "         -1.2502e-01, -1.2765e-01,  1.7830e-01, -3.1311e-02,  8.6929e-02,\n",
            "         -3.5704e-02,  1.2051e-01, -4.3428e-02, -1.2294e-01,  1.3583e-01],\n",
            "        [-8.7338e-02,  4.5554e-02, -3.7694e-02,  1.8229e-01,  3.7297e-02,\n",
            "         -4.4091e-02,  5.5953e-02,  1.4041e-01, -1.2216e-01,  1.1264e-01,\n",
            "          1.8199e-01, -7.7902e-02, -5.3841e-02, -3.3260e-02,  1.8807e-02,\n",
            "         -9.2009e-02, -1.6190e-01,  7.0638e-02,  3.3134e-02, -8.7749e-02,\n",
            "          6.2125e-02,  1.1452e-01,  2.0772e-02,  7.0628e-03,  7.5365e-02,\n",
            "         -1.6139e-01, -1.2418e-01,  4.3718e-02, -1.0107e-01,  1.6968e-01],\n",
            "        [-5.8873e-02, -1.4804e-01, -9.2603e-02, -1.9626e-02, -8.7975e-02,\n",
            "          7.4040e-02,  2.2591e-02, -1.2827e-01,  3.3492e-02,  1.3154e-01,\n",
            "          5.9562e-03, -1.7606e-01,  7.4319e-02, -1.7905e-01, -6.0711e-02,\n",
            "          6.7206e-02, -1.6789e-01,  5.3757e-02,  9.8079e-02,  1.5656e-01,\n",
            "          6.6243e-02, -1.6121e-02,  6.7246e-02, -1.1196e-01, -3.3648e-02,\n",
            "         -1.1629e-01, -6.5418e-02, -1.6427e-01, -1.0178e-01, -1.6825e-02],\n",
            "        [ 4.3667e-02, -1.0633e-01,  1.6324e-01,  1.1975e-01,  1.4478e-01,\n",
            "         -1.7948e-02, -4.1938e-02, -1.2280e-01, -6.3642e-02, -3.0262e-02,\n",
            "          6.7827e-02, -6.4390e-02,  1.3669e-01,  1.0732e-01,  1.3488e-01,\n",
            "          7.2901e-02, -1.5806e-01,  1.6297e-01, -2.5397e-03,  1.7322e-01,\n",
            "          1.6979e-01,  3.2193e-02,  8.4038e-02, -1.0434e-01,  1.6646e-01,\n",
            "         -1.5681e-01,  1.2339e-01, -6.7869e-02,  1.7659e-01,  2.9252e-04],\n",
            "        [-9.9509e-02,  7.5632e-02,  2.5662e-02,  1.0011e-01, -1.0985e-01,\n",
            "         -8.1406e-02, -5.7754e-03, -5.0705e-02,  1.2672e-01,  6.5627e-02,\n",
            "         -3.9199e-02,  1.0205e-01,  1.7768e-01, -8.7478e-02,  1.1971e-01,\n",
            "         -1.1522e-01, -1.2282e-01, -1.6962e-01,  2.4679e-02,  1.4132e-01,\n",
            "         -1.7488e-01, -1.3757e-01,  1.1106e-01, -9.0416e-02,  9.1762e-02,\n",
            "         -3.0935e-02,  1.3043e-01,  2.6914e-02,  1.3586e-01, -7.7098e-02],\n",
            "        [-6.4123e-02, -7.3031e-02,  1.3940e-01,  1.7168e-02, -1.2868e-01,\n",
            "          9.8793e-02,  1.1433e-01, -1.1358e-01,  8.7645e-03, -1.7513e-01,\n",
            "          2.3224e-02,  6.4231e-02,  1.6339e-01, -5.9178e-02,  6.5853e-02,\n",
            "         -6.1179e-02, -6.4203e-02, -1.6178e-01,  3.8537e-02,  1.2251e-01,\n",
            "         -1.0157e-01,  7.4643e-02, -2.8479e-02, -1.2963e-01,  1.9468e-02,\n",
            "         -1.7792e-01,  9.7872e-02, -4.0340e-02,  1.6815e-01,  6.7399e-03],\n",
            "        [-1.7700e-01,  1.4168e-01, -3.9935e-02, -1.2506e-01,  1.6857e-01,\n",
            "          1.7052e-02,  5.9193e-02, -1.5794e-01, -6.4479e-02,  1.4649e-01,\n",
            "         -1.2420e-01, -1.4749e-01,  1.4326e-01, -3.1483e-02, -1.3886e-02,\n",
            "          1.2950e-01,  5.7224e-02, -9.3365e-02, -4.4930e-02,  1.1780e-01,\n",
            "         -5.8706e-02,  1.4606e-01,  1.6647e-01,  1.0949e-01, -1.5390e-02,\n",
            "         -8.1016e-02, -1.0014e-01, -1.4818e-01,  1.1121e-03,  1.1822e-01],\n",
            "        [ 7.4134e-02,  1.0339e-02, -1.7135e-01,  8.7192e-02, -4.2253e-02,\n",
            "          2.8489e-02, -1.1050e-01,  2.6522e-02,  4.4801e-02, -4.6616e-02,\n",
            "         -6.4001e-02,  5.5335e-02,  2.6179e-02,  1.2655e-01, -1.3652e-01,\n",
            "          1.7902e-01, -1.6983e-01, -1.3433e-01, -3.2193e-02, -1.5858e-01,\n",
            "          1.2315e-01, -3.1973e-02,  7.0793e-03,  5.9091e-02,  2.0649e-02,\n",
            "          1.5582e-01,  1.3103e-01,  6.2131e-02, -1.7515e-01,  6.8660e-02]],\n",
            "       requires_grad=True) Parameter containing:\n",
            "tensor([-0.0301,  0.0310,  0.1400, -0.0861, -0.0116, -0.1509,  0.0662, -0.0400,\n",
            "        -0.0674,  0.0101,  0.0086,  0.0333,  0.1693,  0.1364,  0.1509, -0.1043,\n",
            "        -0.1729,  0.1331,  0.1572,  0.1100], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.1692, -0.0858,  0.2063, -0.1134, -0.1537,  0.0388,  0.1798, -0.0861,\n",
            "         -0.0695, -0.0770, -0.1165,  0.0031, -0.0258,  0.0932, -0.1665, -0.0642,\n",
            "         -0.2058, -0.2228,  0.0619, -0.0289],\n",
            "        [-0.2011, -0.0067,  0.0681,  0.1624,  0.1344, -0.2128,  0.0601, -0.2104,\n",
            "          0.1657,  0.1458, -0.2062,  0.0016,  0.0697,  0.0092, -0.2019,  0.2094,\n",
            "         -0.1101,  0.0695,  0.1398, -0.2045],\n",
            "        [-0.0663, -0.1034,  0.2191, -0.0906, -0.1899, -0.2023, -0.0230,  0.1957,\n",
            "         -0.1776, -0.1351, -0.2014,  0.2151,  0.1206,  0.0938,  0.1765,  0.0676,\n",
            "          0.1493, -0.0508,  0.0346, -0.1222]], requires_grad=True) Parameter containing:\n",
            "tensor([-0.0535,  0.0220, -0.1022], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}