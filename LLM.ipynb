{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUkw489t01lSJsk7gpAW7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridhartroy/AIML/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kdGSy1D7_h3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c7bb48-e7ac-4698-e1d9-db66c669c5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the file is :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# Read a publicly available text file from a URL.\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = (\"the-verdict.txt\")\n",
        "\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(\"Length of the file is : \", len(text))\n",
        "\n",
        "print(text[:99])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the text that was just read using reg expressions and print the length of the text before and after split\n",
        "\n",
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed), len(text))\n",
        "\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTSEtmq6SH0",
        "outputId": "eb9c712b-2af3-443c-f80f-65bc10be5bdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690 20479\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this step , we need to sort the tokenized text, remove dups, and assign an unique integer for each token.\n",
        "\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size, type(all_words))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek78eVxzCwQP",
        "outputId": "1e70f455-a531-448b-9416-64ae7d0627f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130 <class 'list'>\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Class that takes in the vocab that we created. And also, we send a sample new text for tokenization and encoding to an unique integer id and then decode as well.\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "EKyP1B4oJJo2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the above class by instantiating it with the vocabulary we created earlier from the verdict corpus. And then encode and decode\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "print(tokenizer.decode(ids))\n",
        "\n",
        "text1 = \"\"\"\"Mrs. said pride.\"\"\"\n",
        "ids1 = tokenizer.encode(text1)\n",
        "print(len(ids1), ids1)\n",
        "\n",
        "print(tokenizer.decode(ids1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZmZBq3nLw5j",
        "outputId": "399f8a64-da5e-4f7d-a5ee-8121cbba4a1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "6 [1, 67, 7, 851, 793, 7]\n",
            "\" Mrs. said pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what about words or tokens not in the corupus like below?\n",
        "\n",
        "text2 = \"\"\"\"Mr. Sridhar said pride.\"\"\"\n",
        "ids2 = tokenizer.encode(text2)\n",
        "print(len(ids2), ids2)\n",
        "\n",
        "print(tokenizer.decode(ids2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "EgsnlXziO5iG",
        "outputId": "fd517eab-6444-4514-db14-b088eb8436fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Sridhar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5c4387239cc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"\"Mr. Sridhar said pride.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mids2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5633fe271064>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5633fe271064>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Sridhar'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to add some additional tokens for a. unknown b. end of source text\n",
        "\n",
        "all_tokens = sorted(set(preprocessed))\n",
        "print(len(all_tokens))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "print(len(all_tokens))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab))\n",
        "\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVn4KnFopy_",
        "outputId": "c0cc494b-e2d6-40ff-eb0e-1bbb23623ef9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "1132\n",
            "1132\n",
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now need to modify the tokenizer custom class to include above\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        print(\"Preprocessed before token check : \" , preprocessed)\n",
        "        # now check for each token in the preprocessed against the vocab.\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                             else \"<|unk|>\"\n",
        "                        for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "k5Q9kyRhXfWu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the new tokenizer class\n",
        "\n",
        "# with existing valid text matching tokens in the vocab\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# 2 unrelated texts mixed with unknown tokens\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOq9HmF-ZhGr",
        "outputId": "2a464a65-5b92-4298-e219-13cecdd461b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed before token check :  ['\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n",
            "Preprocessed before token check :  ['the', 'last', 'he', 'painted', ',', 'Sridhar', '<|endoftext|>', 'Hello', ',', 'do', 'you', 'like', 'tea', '?']\n",
            "14 [988, 602, 533, 746, 5, 1131, 1130, 1131, 5, 355, 1126, 628, 975, 10]\n",
            "the last he painted, <|unk|> <|endoftext|> <|unk|>, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using Byte Pair Encoding algorithm for Tokenization\n",
        "!pip install tiktoken\n",
        "\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yetXDTPedlWk",
        "outputId": "1ea2d4bd-64c4-4b9d-fd88-ee8c8b4fba54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEuDwprsIWTH",
        "outputId": "fc3d9b4d-6836-4815-98e2-4567ca280f3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1169, 938, 339, 13055, 11, 311, 6058, 9869, 220, 50256, 18435, 11, 466, 345, 588, 8887, 30]\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TextIO\n",
        "# Exercise 2.1 Byte pair encoding of unknown words\n",
        "\"\"\"\n",
        "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and print the individual token IDs. Then, call the decode function on each of the resulting integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input, “Akwirw ier.”\n",
        "\"\"\"\n",
        "tokenizerR50 = tiktoken.get_encoding(\"r50k_base\")\n",
        "tokenizerP50 = tiktoken.get_encoding(\"p50k_base\")\n",
        "tokenizerCl100k = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokenizero200k = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "text = \"Akwirw ier\"\n",
        "\n",
        "integers = tokenizerR50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"R50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerR50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerR50.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizerP50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"P50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerP50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerP50.decode(integers))\n",
        "\n",
        "integers = tokenizerCl100k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"cl100k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerCl100k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerCl100k.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizero200k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"o200k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizero200k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizero200k.decode(integers))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aL-zVEcsuOg",
        "outputId": "6124f625-dc89-4639-c0c6-9df20c143c85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "P50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "cl100k  [32, 29700, 404, 86, 602, 261] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 29700\n",
            "ir --> 404\n",
            "w --> 86\n",
            " i --> 602\n",
            "er --> 261\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "o200k  [32, 9500, 380, 86, 131455] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 9500\n",
            "ir --> 380\n",
            "w --> 86\n",
            " ier --> 131455\n",
            "---------------------------------\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# print(raw_text)\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text), type(enc_text))\n",
        "\n",
        "# do a sampling for 50 tokens\n",
        "\n",
        "enc_sample = enc_text[50:]\n",
        "#print(enc_sample)\n",
        "#print(tokenizer.decode(enc_sample))\n",
        "\n",
        "context_size = 10\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, \"---->\", desired)\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo4YfNSsSinl",
        "outputId": "08362d91-c8e3-45d0-9c79-f7c0e30d6798"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145 <class 'list'>\n",
            "x: [290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n",
            "y:      [4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976]\n",
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n",
            "[290, 4920, 2241, 287, 257] ----> 4489\n",
            "[290, 4920, 2241, 287, 257, 4489] ----> 64\n",
            "[290, 4920, 2241, 287, 257, 4489, 64] ----> 319\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319] ----> 262\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262] ----> 34686\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686] ----> 41976\n",
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n",
            " and established himself in a ---->  vill\n",
            " and established himself in a vill ----> a\n",
            " and established himself in a villa ---->  on\n",
            " and established himself in a villa on ---->  the\n",
            " and established himself in a villa on the ---->  Riv\n",
            " and established himself in a villa on the Riv ----> iera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "btuivItP8qqm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==2.4.0"
      ],
      "metadata": {
        "id": "6gf_tuVvg4KG",
        "outputId": "f67a2291-4ddd-4009-fbd7-814d42761e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "6f09785d9029435d96f1527b51e0cfdb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__\n",
        "#torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "w6jTzmraiGVS",
        "outputId": "038f4f56-4662-44a0-a215-d649ebf99ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor0d = torch.tensor(1)\n",
        "\n",
        "tensor1d = torch.tensor([1.1, 2, 3])\n",
        "\n",
        "tensor2d = torch.tensor([[1, 2,3],\n",
        "                         [3, 4,5]])\n",
        "\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]],\n",
        "                         [[5, 6], [7, 8]]])\n",
        "\n",
        "\n",
        "print(tensor0d)\n",
        "print(tensor1d)\n",
        "print(tensor2d)\n",
        "print(tensor3d)\n",
        "\n",
        "print(tensor1d.dtype)\n",
        "\n",
        "tensor0d = torch.tensor([1, 2, 3])\n",
        "print(tensor0d.dtype)\n",
        "\n",
        "tensor0df = tensor0d.to(torch.float32)\n",
        "print(tensor0df.dtype)\n",
        "print(tensor0d)\n",
        "print(tensor0df)"
      ],
      "metadata": {
        "id": "feB-C38YkqIM",
        "outputId": "21204af0-47da-4ced-e362-964642476c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1)\n",
            "tensor([1.1000, 2.0000, 3.0000])\n",
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.float32\n",
            "tensor([1, 2, 3])\n",
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor0d, tensor0d.shape)\n",
        "print(tensor1d, tensor1d.shape)\n",
        "print(tensor2d, tensor2d.shape)\n",
        "print(tensor3d, tensor3d.shape)\n",
        "\n",
        "print(tensor2d.reshape(3, 2))\n",
        "\n",
        "print(tensor2d.view(3, 2))\n",
        "\n",
        "\n",
        "print(tensor2d.T)"
      ],
      "metadata": {
        "id": "20Z21rH1mzzz",
        "outputId": "161d22e9-edc0-4dfd-c368-9829d91fab7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) torch.Size([3])\n",
            "tensor([1.1000, 2.0000, 3.0000]) torch.Size([3])\n",
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]]) torch.Size([2, 3])\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]]) torch.Size([2, 2, 2])\n",
            "tensor([[1, 2],\n",
            "        [3, 3],\n",
            "        [4, 5]])\n",
            "tensor([[1, 2],\n",
            "        [3, 3],\n",
            "        [4, 5]])\n",
            "tensor([[1, 3],\n",
            "        [2, 4],\n",
            "        [3, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor2d)\n",
        "print(\"**\")\n",
        "print(tensor2d.T)\n",
        "print(\"MatMul\")\n",
        "print(tensor2d.matmul(tensor2d.T))\n",
        "print(tensor2d @ tensor2d.T)"
      ],
      "metadata": {
        "id": "cRCoMxfhoLvg",
        "outputId": "4b5fa483-8a70-480e-e4fb-75beb9389147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [3, 4, 5]])\n",
            "**\n",
            "tensor([[1, 3],\n",
            "        [2, 4],\n",
            "        [3, 5]])\n",
            "MatMul\n",
            "tensor([[14, 26],\n",
            "        [26, 50]])\n",
            "tensor([[14, 26],\n",
            "        [26, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing models as computational graphs.\n",
        "#  A logistic regression forward pass.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2])\n",
        "b = torch.tensor([0.0])\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "loss = F.binary_cross_entropy(a, y)"
      ],
      "metadata": {
        "id": "1FI38V8ipahX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the gradients via autograd function of torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)\n",
        "\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)\n",
        "\n",
        "print('******************************')\n",
        "\n",
        "loss.backward()\n",
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "iKZVPHmf2zHY",
        "outputId": "9fdee78c-62b8-420d-e24d-ed2854a0d62d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n",
            "******************************\n",
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a multi-layer perceptron with 2 hidden layers\n",
        "import torch.nn as M\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "01nQogMI_imX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the above neural network\n",
        "\n",
        "model = NeuralNetwork(50, 3)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# No. of trainable parameters of this model.\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of trainable model parameters:\", num_params)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total number of model parameters:\", total_params)"
      ],
      "metadata": {
        "id": "kE1ZAEVQDTLO",
        "outputId": "2dda916c-4e8d-4169-c0b8-ce7435b8ecef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "Total number of trainable model parameters: 2213\n",
            "Total number of model parameters: 2213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's print out the weight tensor\n",
        "\n",
        "print(\"Weight Matrix\")\n",
        "print(model.layers[0].weight, model.layers[0].weight.shape, type(model.layers), type(model.layers[0]), (model.layers[0].weight.dtype))\n",
        "\n",
        "print(\"Bias Vector\")\n",
        "print(model.layers[0].bias, model.layers[0].bias.shape, type(model.layers), type(model.layers[0]), (model.layers[0].bias.dtype))"
      ],
      "metadata": {
        "id": "Bf44s1RSIFDR",
        "outputId": "2899ce4f-260a-4c0b-85b3-d49ab7fec2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Matrix\n",
            "Parameter containing:\n",
            "tensor([[-0.0741,  0.0632, -0.0974,  ..., -0.0924, -0.0670,  0.1106],\n",
            "        [-0.0389,  0.0492,  0.0189,  ...,  0.0428, -0.0587,  0.1368],\n",
            "        [ 0.1300, -0.0700, -0.0764,  ...,  0.0104,  0.0652, -0.0268],\n",
            "        ...,\n",
            "        [-0.0443,  0.1052, -0.1045,  ..., -0.0254, -0.0851, -0.1159],\n",
            "        [ 0.0390,  0.0563, -0.1093,  ...,  0.1339, -0.0948, -0.1266],\n",
            "        [ 0.0563, -0.1128,  0.1233,  ...,  0.0505, -0.1036, -0.0256]],\n",
            "       requires_grad=True) torch.Size([30, 50]) <class 'torch.nn.modules.container.Sequential'> <class 'torch.nn.modules.linear.Linear'> torch.float32\n",
            "Bias Vector\n",
            "Parameter containing:\n",
            "tensor([-0.0555,  0.1186,  0.0324, -0.1362,  0.0539,  0.0452,  0.0600, -0.0157,\n",
            "         0.0115,  0.1307, -0.0572,  0.0053,  0.1071,  0.1171, -0.0974, -0.0653,\n",
            "         0.0838,  0.0682, -0.0987, -0.1102, -0.1250,  0.0838,  0.1392,  0.0116,\n",
            "        -0.0628, -0.0251,  0.0526,  0.0467,  0.1049,  0.1102],\n",
            "       requires_grad=True) torch.Size([30]) <class 'torch.nn.modules.container.Sequential'> <class 'torch.nn.modules.linear.Linear'> torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model)\n",
        "print(model.layers[0].weight.shape, model.layers[0].bias.shape)\n",
        "print(model.layers[2].weight.shape, model.layers[2].bias.shape)\n",
        "print(model.layers[4].weight.shape, model.layers[4].bias.shape)"
      ],
      "metadata": {
        "id": "UH397K9gMP-E",
        "outputId": "8a3e3f7f-6313-44c1-a740-5895a121c4e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([30, 50]) torch.Size([30])\n",
            "torch.Size([20, 30]) torch.Size([20])\n",
            "torch.Size([3, 20]) torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "X = torch.rand((1, 50))\n",
        "print(X)\n",
        "out = torch.softmax(model(X), dim=1)\n",
        "out1 = model(X)\n",
        "print(out)\n",
        "print(out1)\n",
        "\n",
        "print(model.layers[0].weight.shape, model.layers[0].bias.shape)\n",
        "print(model.layers[2].weight.shape, model.layers[2].bias.shape)\n",
        "print(model.layers[4].weight.shape, model.layers[4].bias.shape)\n"
      ],
      "metadata": {
        "id": "k9a-6AU0Mljd",
        "outputId": "8418f779-4fc1-42d8-ab9a-9a9636bb5d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665, 0.1366, 0.1025, 0.1841,\n",
            "         0.7264, 0.3153, 0.6871, 0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274,\n",
            "         0.3821, 0.6605, 0.8536, 0.5932, 0.6367, 0.9826, 0.2745, 0.6584, 0.2775,\n",
            "         0.8573, 0.8993, 0.0390, 0.9268, 0.7388, 0.7179, 0.7058, 0.9156, 0.4340,\n",
            "         0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737, 0.4606,\n",
            "         0.5159, 0.4220, 0.5786, 0.9455, 0.8057]])\n",
            "tensor([[0.3113, 0.3934, 0.2952]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([30, 50]) torch.Size([30])\n",
            "torch.Size([20, 30]) torch.Size([20])\n",
            "torch.Size([3, 20]) torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = model(X)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMwYOQMwMqvO",
        "outputId": "777e4402-92c9-4555-8279-35a339c1e4f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1262,  0.1080, -0.1792]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = torch.softmax(model(X), dim=1)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7NoXvxqNUUn",
        "outputId": "a2d17a5a-bf08-4dd9-c16f-0d4a5ea6b78b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3113, 0.3934, 0.2952]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up efficient data loaders and data sets\n",
        "\n",
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6],\n",
        "])\n",
        "y_test = torch.tensor([0, 1])\n"
      ],
      "metadata": {
        "id": "kaHrloBoPtYv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "        print(type(self.features), type(self.labels))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "print(X_train, X_train.shape)\n",
        "print(y_train, y_train.shape)\n",
        "print(train_ds.features, train_ds.labels, train_ds.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrxPRipSQz9p",
        "outputId": "c1ed4eec-2991-496d-9b81-c5b4444f5395"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([[-1.2000,  3.1000],\n",
            "        [-0.9000,  2.9000],\n",
            "        [-0.5000,  2.6000],\n",
            "        [ 2.3000, -1.1000],\n",
            "        [ 2.7000, -1.5000]]) torch.Size([5, 2])\n",
            "tensor([0, 0, 0, 1, 1]) torch.Size([5])\n",
            "tensor([[-1.2000,  3.1000],\n",
            "        [-0.9000,  2.9000],\n",
            "        [-0.5000,  2.6000],\n",
            "        [ 2.3000, -1.1000],\n",
            "        [ 2.7000, -1.5000]]) tensor([0, 0, 0, 1, 1]) 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(train_ds.features, train_ds.labels)\n",
        "print(\"**************************************\")\n",
        "\n",
        "en=enumerate(train_loader)\n",
        "print(en)\n",
        "print(\"**************************************\")\n",
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nk2vbdZUt4l",
        "outputId": "4b569ff8-9c52-4ee5-acd3-c56dfac8443e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2000,  3.1000],\n",
            "        [-0.9000,  2.9000],\n",
            "        [-0.5000,  2.6000],\n",
            "        [ 2.3000, -1.1000],\n",
            "        [ 2.7000, -1.5000]]) tensor([0, 0, 0, 1, 1])\n",
            "**************************************\n",
            "<enumerate object at 0x7f913b0bc220>\n",
            "**************************************\n",
            "Batch 1: tensor([[ 2.7000, -1.5000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
            "Batch 2: tensor([[ 2.3000, -1.1000],\n",
            "        [-1.2000,  3.1000]]) tensor([1, 0])\n",
            "Batch 3: tensor([[-0.5000,  2.6000]]) tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApRPy8mpbnnq",
        "outputId": "e21546db-d779-464c-cbb8-0d21f9b437b2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: tensor([[-0.5000,  2.6000],\n",
            "        [-0.9000,  2.9000]]) tensor([0, 0])\n",
            "Batch 2: tensor([[-1.2000,  3.1000],\n",
            "        [ 2.3000, -1.1000]]) tensor([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have a dataset and dataloader defined. Let's try to use these to train the sample model that we had instantiated few cells above.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=0.5\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        logits = model(features)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### LOGGING\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # Insert optional model evaluation code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4BK8BSLKdfl",
        "outputId": "cc15f480-8da6-4663-bc08-16756adfbf4a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75\n",
            "Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65\n",
            "Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44\n",
            "Epoch: 002/003 | Batch 001/002 | Train Loss: 0.13\n",
            "Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03\n",
            "Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Total number of trainable model parameters:\", num_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1aGBdEPQdNE",
        "outputId": "2b10871d-aacc-4e6a-caf1-c03a56226e39"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable model parameters: 752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# above model is trained with x_train and y_train\n",
        "# now do a test with the entire training set. typically we use a validation dataset and then a test dataset\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "print(outputs)\n",
        "\n",
        "\n",
        "# apply softmax to get probabilities\n",
        "\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "probas = torch.softmax(outputs, dim=1)\n",
        "print(probas)\n",
        "\n",
        "# convert to 0s and 1s using argmax\n",
        "\n",
        "predictions = torch.argmax(probas, dim=1)\n",
        "print(predictions)\n",
        "\n",
        "predictions = torch.argmax(outputs, dim=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ewS4JKrUJLm",
        "outputId": "3428bc72-d356-4fde-9210-1afdc927338c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.8569, -4.1618],\n",
            "        [ 2.5382, -3.7548],\n",
            "        [ 2.0944, -3.1820],\n",
            "        [-1.4814,  1.4816],\n",
            "        [-1.7176,  1.7342]])\n",
            "tensor([[    0.9991,     0.0009],\n",
            "        [    0.9982,     0.0018],\n",
            "        [    0.9949,     0.0051],\n",
            "        [    0.0491,     0.9509],\n",
            "        [    0.0307,     0.9693]])\n",
            "tensor([0, 0, 0, 1, 1])\n",
            "tensor([0, 0, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "\n",
        "predictions == y_train\n",
        "\n",
        "# no. of correct predictions\n",
        "\n",
        "torch.sum(predictions == y_train)\n",
        "\n",
        "# function to compute the prediction accuracy\n",
        "\n",
        "def compute_accuracy(model, dataloader):\n",
        "\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(features)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "\n",
        "    return (correct / total_examples).item()"
      ],
      "metadata": {
        "id": "1LfbfTI3Wk46"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the accuracy of prediction from above function for our sample model and dataset\n",
        "\n",
        "#on training set\n",
        "print(\"Training Accuracy : \", compute_accuracy(model, train_loader))\n",
        "\n",
        "#on testing set\n",
        "print(\"Test Accuracy : \", compute_accuracy(model, test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBI3V4DhXZrr",
        "outputId": "f6b643f0-5fba-4ff6-f431-49119029db9c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy :  1.0\n",
            "Test Accuracy :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Continuing from cell 14...we need now to convert the token ids that the bpe tokernizer created to embeddings\n",
        "\n",
        "# Instantiate Dataset\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt) # use the tiktoken to tokenize the entire text\n",
        "        print(len(token_ids))\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride): #chunking the token ids into overlapping sequences of max_lenght to create sliding window. And convert the list chunks to a tensor\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "           # print(self.input_ids[i], self.target_ids[i])\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "JXyPdsyEZ5Yy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# following code uses the above dataset to load the inputs in batches via the dataloader\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size, max_length,\n",
        "                         stride, shuffle, drop_last=True,\n",
        "                         num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\") #use openai tiktoken and use gpt2\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #create the dataset. this will have 2 lists of tensors\n",
        "    #print(len(dataset.target_ids))\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "RiMdDbpU2LLB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "print(first_batch)\n",
        "\n",
        "#tokenizer.decode(first_batch[1])\n",
        "\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)\n",
        "\n",
        "\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=2, stride=2, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "print(first_batch)\n",
        "\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)\n",
        "\n",
        "\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "\n",
        "print(first_batch)\n",
        "\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNWH3u6z8Gi5",
        "outputId": "b4a0c00d-d222-4a5f-b9ae-a2d2d8e7e42b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
            "5145\n",
            "[tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n",
            "[tensor([[2885, 1464]]), tensor([[1464, 1807]])]\n",
            "5145\n",
            "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
            "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "\n",
        "# NOTE : Note that we increase the stride to 4 to utilize the data set fully (we don’t skip a single word). This avoids any overlap between the batches since more overlap could lead to increased overfitting."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLV3K0HqUqGD",
        "outputId": "99390ece-8a0a-4e21-a2b3-81b9b9839b14"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next converting token ids to token emeddings. Preparation involves tokenizing text, converting text tokens to token IDs, and converting token IDs into embedding vectors. Here, we consider the previously created token IDs to create the token embedding vectors.\n",
        "\n",
        "\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "vocab_size = 6 # small corpus\n",
        "output_dim = 3 # small dimension for each embedding / token\n",
        "# Note. BPE has vocab of 50,257. And each embedding vector has dim of 12,288\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(input_ids)\n",
        "print(\"*****************************************************************\")\n",
        "print(embedding_layer.weight)\n",
        "print(\"*****************************************************************\")\n",
        "# There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
        "\n",
        "\n",
        "#let’s apply it to a token ID to obtain the embedding vector:\n",
        "\n",
        "print(embedding_layer(torch.tensor([3])))\n",
        "print(\"*****************************************************************\")\n",
        "# In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.\n",
        "\n",
        "print(embedding_layer(input_ids))\n",
        "print(\"*****************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7KvV-jWYMx6",
        "outputId": "ff706af0-4208-43c8-b64c-91d4cf2926d7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 3, 5, 1])\n",
            "*****************************************************************\n",
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
            "*****************************************************************\n",
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
            "*****************************************************************\n",
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n",
            "*****************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Having now created embedding vectors from token IDs, next we’ll add a small modification to these embedding vectors to encode positional information about a token within a text.\n",
        "\n",
        "# let's extend the dimensions to 128 and also use the vocab of the tokenizer bpe\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "   stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "\n",
        "print(\"Targets IDs:\\n\", targets)\n",
        "print(\"\\nTargets shape:\\n\", targets.shape)\n",
        "\n",
        "\n",
        "#As we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch consists of eight text samples with four tokens each.\n",
        "\n",
        "\n",
        "\n",
        "#Let’s now use the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(\"\\n Embeddings shape : \\n\", token_embeddings.shape)\n",
        "\n",
        "# The embedding shape 8x4x256 means each token id is now converted to a 256 dim vector\n",
        "\n",
        "# now we need to add absolute (instead of relative) position embedding. It should be of size 4x256\n",
        "\n",
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(\"\\n Positional Embeddings shape : \\n\", pos_embeddings.shape)\n",
        "\n",
        "print(pos_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LhdUlUlgEPH",
        "outputId": "faa5ce1a-d14e-482b-9a20-98996adbb7b9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "Targets IDs:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "\n",
            "Targets shape:\n",
            " torch.Size([8, 4])\n",
            "\n",
            " Embeddings shape : \n",
            " torch.Size([8, 4, 256])\n",
            "\n",
            " Positional Embeddings shape : \n",
            " torch.Size([4, 256])\n",
            "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
            "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
            "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
            "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#he positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token embedding tensor in each of the eight batches:\n",
        "\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(\"\\n Input Embedding : \\n\", input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQeN2eb-uEA1",
        "outputId": "0f5121bc-062d-401c-eac2-ad163e81a4ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input Embedding : \n",
            " torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coding Attention Mechanism\n",
        "\n",
        "# Simplified self-attention without trainable weights\n",
        "\n",
        "import torch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "# intermediary attention score vector w that takes the query vector x^2 and dot product with other input elements.\n",
        "\n",
        "query = inputs[1]\n",
        "print(query, inputs.shape)\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "print(attn_scores_2, attn_scores_2.shape)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "print(attn_scores_2)\n",
        "\n",
        "#print(torch.dot(torch.tensor([0.4, 0.1, 0.8]), torch.tensor([0.5, 0.8, 0.6])))"
      ],
      "metadata": {
        "id": "r3MxARGLuMwE",
        "outputId": "07d3bd0e-8976-482a-9852-e85d55dba583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5500, 0.8700, 0.6600]) torch.Size([6, 3])\n",
            "tensor([    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]) torch.Size([6])\n",
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    }
  ]
}