{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS2Ap6EplaEatJ+Q4lDhqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridhartroy/AIML/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kdGSy1D7_h3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f8db74-5a7a-40a0-93ef-8bb732a70589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the file is :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# Read a publicly available text file from a URL.\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = (\"the-verdict.txt\")\n",
        "\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(\"Length of the file is : \", len(text))\n",
        "\n",
        "print(text[:99])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the text that was just read using reg expressions and print the length of the text before and after split\n",
        "\n",
        "import re\n",
        "\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed), len(text))\n",
        "\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTSEtmq6SH0",
        "outputId": "a319b0fa-fb9c-4fa2-a166-9706602bc5a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690 20479\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this step , we need to sort the tokenized text, remove dups, and assign an unique integer for each token.\n",
        "\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size, type(all_words))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek78eVxzCwQP",
        "outputId": "f1684434-ed48-435e-a86e-d6e3e6e38374"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130 <class 'list'>\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Class that takes in the vocab that we created. And also, we send a sample new text for tokenization and encoding to an unique integer id and then decode as well.\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "EKyP1B4oJJo2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the above class by instantiating it with the vocabulary we created earlier from the verdict corpus. And then encode and decode\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "print(tokenizer.decode(ids))\n",
        "\n",
        "text1 = \"\"\"\"Mrs. said pride.\"\"\"\n",
        "ids1 = tokenizer.encode(text1)\n",
        "print(len(ids1), ids1)\n",
        "\n",
        "print(tokenizer.decode(ids1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZmZBq3nLw5j",
        "outputId": "28a0ac34-af55-4f57-9b86-62bf2c1b7c41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "6 [1, 67, 7, 851, 793, 7]\n",
            "\" Mrs. said pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what about words or tokens not in the corupus like below?\n",
        "\n",
        "text2 = \"\"\"\"Mr. Sridhar said pride.\"\"\"\n",
        "ids2 = tokenizer.encode(text2)\n",
        "print(len(ids2), ids2)\n",
        "\n",
        "print(tokenizer.decode(ids2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EgsnlXziO5iG",
        "outputId": "79e2f0e3-5ed4-4e81-bd53-93b06d20d5e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Sridhar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5c4387239cc2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"\"Mr. Sridhar said pride.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mids2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5633fe271064>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5633fe271064>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# creating a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Sridhar'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to add some additional tokens for a. unknown b. end of source text\n",
        "\n",
        "all_tokens = sorted(set(preprocessed))\n",
        "print(len(all_tokens))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "print(len(all_tokens))\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab))\n",
        "\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVn4KnFopy_",
        "outputId": "71a907f5-b133-46fa-ffe4-dbce8af0ae49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "1132\n",
            "1132\n",
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now need to modify the tokenizer custom class to include above\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # vocab is a dictionary and hence str_to_int is a dictionary as well\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "       # print(self.str_to_int)\n",
        "\n",
        "    def encode(self, text): #new input text\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        print(\"Preprocessed before token check : \" , preprocessed)\n",
        "        # now check for each token in the preprocessed against the vocab.\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                             else \"<|unk|>\"\n",
        "                        for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed] # creating a list\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "k5Q9kyRhXfWu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the new tokenizer class\n",
        "\n",
        "# with existing valid text matching tokens in the vocab\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# 2 unrelated texts mixed with unknown tokens\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)\n",
        "ids = tokenizer.encode(text)\n",
        "print(len(ids), ids)\n",
        "\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOq9HmF-ZhGr",
        "outputId": "7a5e19f1-377b-4486-cbc9-452205cc79c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed before token check :  ['\"', 'It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
            "21 [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n",
            "Preprocessed before token check :  ['the', 'last', 'he', 'painted', ',', 'Sridhar', '<|endoftext|>', 'Hello', ',', 'do', 'you', 'like', 'tea', '?']\n",
            "14 [988, 602, 533, 746, 5, 1131, 1130, 1131, 5, 355, 1126, 628, 975, 10]\n",
            "the last he painted, <|unk|> <|endoftext|> <|unk|>, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using Byte Pair Encoding algorithm for Tokenization\n",
        "!pip install tiktoken\n",
        "\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yetXDTPedlWk",
        "outputId": "70f53e07-3f43-434c-a662-9c01202f7b40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "text1 = \"the last he painted, Sridhar\"\n",
        "text2 = \"Hello, do you like tea?\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEuDwprsIWTH",
        "outputId": "43ae5a7f-3508-427d-a4ff-2727f3df8fc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1169, 938, 339, 13055, 11, 311, 6058, 9869, 220, 50256, 18435, 11, 466, 345, 588, 8887, 30]\n",
            "the last he painted, Sridhar <|endoftext|> Hello, do you like tea?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TextIO\n",
        "# Exercise 2.1 Byte pair encoding of unknown words\n",
        "\"\"\"\n",
        "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and print the individual token IDs. Then, call the decode function on each of the resulting integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input, “Akwirw ier.”\n",
        "\"\"\"\n",
        "tokenizerR50 = tiktoken.get_encoding(\"r50k_base\")\n",
        "tokenizerP50 = tiktoken.get_encoding(\"p50k_base\")\n",
        "tokenizerCl100k = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokenizero200k = tiktoken.get_encoding(\"o200k_base\")\n",
        "\n",
        "text = \"Akwirw ier\"\n",
        "\n",
        "integers = tokenizerR50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"R50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerR50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerR50.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizerP50.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"P50 \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerP50.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerP50.decode(integers))\n",
        "\n",
        "integers = tokenizerCl100k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"cl100k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizerCl100k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizerCl100k.decode(integers))\n",
        "\n",
        "\n",
        "integers = tokenizero200k.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(\"o200k \", integers, type(integers))\n",
        "\n",
        "for i in integers:\n",
        "    print(tokenizero200k.decode([i]), \"-->\", i)\n",
        "\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(tokenizero200k.decode(integers))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aL-zVEcsuOg",
        "outputId": "43f739d4-f32d-46c2-979e-ce812eacefed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "P50  [33901, 86, 343, 86, 220, 959] <class 'list'>\n",
            "Ak --> 33901\n",
            "w --> 86\n",
            "ir --> 343\n",
            "w --> 86\n",
            "  --> 220\n",
            "ier --> 959\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "cl100k  [32, 29700, 404, 86, 602, 261] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 29700\n",
            "ir --> 404\n",
            "w --> 86\n",
            " i --> 602\n",
            "er --> 261\n",
            "---------------------------------\n",
            "Akwirw ier\n",
            "o200k  [32, 9500, 380, 86, 131455] <class 'list'>\n",
            "A --> 32\n",
            "kw --> 9500\n",
            "ir --> 380\n",
            "w --> 86\n",
            " ier --> 131455\n",
            "---------------------------------\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eo4YfNSsSinl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}